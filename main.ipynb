{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets -q\n",
    "!pip install transformers -q -U\n",
    "!pip install wandb -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import datasets\n",
    "import sys, importlib\n",
    "import random\n",
    "import shutil\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = './misc-data'\n",
    "SECRET_HOME = '/Users/yifu/Documents/Stanford/Academics/Papers/TraumaICD-Paper/injury-icd-dataset/'  # THE DIRECTORY WHERE YOUR PATIENT DATA IS STORED\n",
    "VOCAB_HOME = os.path.join(DATA_HOME, 'icd-codes')\n",
    "SECRET_PATH =  os.path.join(DATA_HOME, 'injury-icd-dataset')\n",
    "DS_HOME_PRETRAIN = os.path.join(DATA_HOME, 'pretrain', 'injury-icd-dataset')\n",
    "PLOT_HOME = os.path.join(DATA_HOME, \"publishing\", \"figures\")\n",
    "RAW_DATA_PATH = os.path.join(SECRET_HOME,\"injury_icd_dataset.csv\")  # YOUR OWN PATIENT DATA CSV FILE HERE. Example Format: \n",
    "EXCLUDE_SUPERFICIAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ THE INJURY CODE VOCABULARY ###\n",
    "\n",
    "icd10_concepts = pd.read_csv(os.path.join(VOCAB_HOME, \"injury_codes_ICD10CM.csv\"), low_memory=False)\n",
    "injuries = icd10_concepts[icd10_concepts[\"concept_code\"].apply(lambda x: len(x)<=5 and x.startswith(\"S\"))]\n",
    "injuries.to_csv(os.path.join(VOCAB_HOME, 'injury_ICD10.csv'))\n",
    "injuries_4_char = injuries[injuries.concept_class_id == '4-char nonbill code']\n",
    "injuries_4_char.to_csv(os.path.join(VOCAB_HOME, 'injury_ICD10_4_char.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ THE ANNOTATED PATIENT DATA ###\n",
    "\n",
    "raw_data = pd.read_csv(RAW_DATA_PATH, on_bad_lines='skip')\n",
    "case_icd_codes = raw_data[['patient_id', 'icd_code', 'icd_name', 'diagnosis_region', 'ais_code', 'ais_name']].copy()\n",
    "case_icd_codes = case_icd_codes[case_icd_codes.icd_code.notnull()]\n",
    "case_icd_codes[\"icd_code_4_char\"] = case_icd_codes.icd_code.apply(lambda x: str(x)[:5])\n",
    "case_icd_codes[\"icd_name_4_char\"] = case_icd_codes.icd_name.apply(lambda x: str(x)[:5])\n",
    "case_icd_codes = case_icd_codes.merge(injuries_4_char.rename(columns={'concept_code': 'icd_code_4_char'}), how='inner')\n",
    "cases = raw_data[(raw_data.tertiary_impression != '') | (raw_data.tertiary_exam != '')][['patient_id', 'tertiary_exam', 'tertiary_imaging_report', 'tertiary_impression']].copy()\n",
    "cases['total_text_len'] = cases.apply(lambda row: len(str(row.tertiary_exam)) + len(str(row.tertiary_imaging_report)) + len(str(row.tertiary_impression)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPLORATORY DATA ANALYSIS ###\n",
    "\n",
    "n_cases_by_4_char_code = case_icd_codes.groupby('icd_code_4_char', as_index=False).agg({'patient_id': 'count'}).rename(columns={'patient_id': 'n_cases'})\n",
    "n_cases_by_4_char_code = n_cases_by_4_char_code[n_cases_by_4_char_code.n_cases > 5]\n",
    "n_cases_by_4_char_code.to_csv(os.path.join(DATA_HOME, 'n_cases_by_4_char_code.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILTER & DEFINE GROUND TRUTH LABELS ###\n",
    "if EXCLUDE_SUPERFICIAL:\n",
    "    ground_truth_labels = n_cases_by_4_char_code[~n_cases_by_4_char_code.icd_code_4_char.str.contains('uperficial')].icd_code_4_char.tolist()\n",
    "else:\n",
    "    ground_truth_labels = n_cases_by_4_char_code.icd_code_4_char.tolist()\n",
    "\n",
    "with open(os.path.join(VOCAB_HOME, 'label.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(ground_truth_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPLIT PATIENTS INTO TRAIN, VALIDATION, TEST SETS ###\n",
    "\n",
    "patient_ids = cases.patient_id.sample(frac=1, random_state=42).unique()\n",
    "\n",
    "train_ratio, validation_ratio, test_ratio = 0.7, 0.15, 0.15\n",
    "\n",
    "train = patient_ids[:int(len(patient_ids)*train_ratio)]\n",
    "validation = patient_ids[int(len(patient_ids)*train_ratio):int(len(patient_ids)*(train_ratio+validation_ratio))]\n",
    "test = patient_ids[int(len(patient_ids)*(train_ratio+validation_ratio)):]\n",
    "\n",
    "\n",
    "with open(os.path.join(DATA_HOME, 'train.txt'), 'w') as f:\n",
    "  f.write('\\n'.join([str(x) for x in train]))\n",
    "\n",
    "with open(os.path.join(DATA_HOME, 'validation.txt'), 'w') as f:\n",
    "  f.write('\\n'.join([str(x) for x in validation]))\n",
    "\n",
    "with open(os.path.join(DATA_HOME, 'test.txt'), 'w') as f:\n",
    "  f.write('\\n'.join([str(x) for x in test]))\n",
    "\n",
    "for x in validation:\n",
    "  assert x not in train\n",
    "  assert x not in test\n",
    "\n",
    "for x in train:\n",
    "  assert x not in validation\n",
    "  assert x not in test\n",
    "\n",
    "for x in test:\n",
    "  assert x not in validation\n",
    "  assert x not in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE TRAINING HYPERPARAMETERS ###\n",
    "\n",
    "valid_labels = \"4-char\"                          #@param [\"4-char\", \"4-char-top50\", \"4-char-top10\", \"5-char\", \"4-and-5-char\"]\n",
    "experiment_name = \"4-char-with-superficial\" #@param {type:\"string\",  allow-input: true} [\"non-sup\", \"non-sup-pretrain\", \"non-sup-tune-after-pretrain\", \"non-sup-4and5-char-train-on-full\", \"non-sup-train-on-full\", \"4-char-with-superficial\"]\n",
    "model_name = \"michiyasunaga/BioLinkBERT-base\"         #@param [\"michiyasunaga/BioLinkBERT-base\", \"michiyasunaga/BioLinkBERT-large\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"]\n",
    "metric_for_best_model = \"eval_f1_score_weighted\"          #@param [\"eval_f1_score_macro\", \"eval_f1_score_micro\", \"eval_f1_score_weighted\", \"eval_auc_score_macro\", \"eval_auc_score_micro\", \"eval_auc_score_weighted\"]\n",
    "num_epochs = \"20\"                 #@param [6, 10, 20, 30]\n",
    "# Whether to train on both train and val dataset, and do final eval on holdout test set\n",
    "train_on_full = True            #@param {type:\"boolean\"} \n",
    "evaluate_only = False           #@param {type:\"boolean\"}\n",
    "learning_rate = \"0.00002\"        #@param [2e-5, 1e-5, 7e-6, 2e-6]\n",
    "warmup_steps =  5000            #@param [1000, 2000, 3000, 5000]\n",
    "per_device_train_batch_size = \"16\" #@param [16, 8, 6, 4]\n",
    "per_device_eval_batch_size = \"32\" #@param [32, 16, 12, 8]\n",
    "learning_rate = float(learning_rate)\n",
    "warmup_steps = int(warmup_steps)\n",
    "per_device_train_batch_size = int(per_device_train_batch_size)\n",
    "per_device_eval_batch_size = int(per_device_eval_batch_size)\n",
    "\n",
    "if \"base\" in model_name:\n",
    "  print(\"The suggested batch size for train and eval of base-sized model on P100 GPU is 16 and 32\")\n",
    "else:\n",
    "  print(\"The suggested batch size for train and eval of large-sized model on P100 GPU is 6 and 12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/Users/yifu/Documents/Stanford/Academics/Papers/TraumaICD-Paper/TraumaICDBERT/train.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model_name=$model_name \\\n",
    "                 --data_dir=$DS_HOME \\\n",
    "                 --model_dir=$MODEL_HOME \\\n",
    "                 --experiment_name=$experiment_name \\\n",
    "                 --valid_labels=$valid_labels \\\n",
    "                 --is_evaluate=$evaluate_only \\\n",
    "                 --train_on_full=$train_on_full \\\n",
    "                 --num_train_epochs=$num_epochs \\\n",
    "                 --metric_for_best_model=$metric_for_best_model \\\n",
    "                 --learning_rate=$learning_rate \\\n",
    "                 --warmup_steps=$warmup_steps \\\n",
    "                 --per_device_train_batch_size=$per_device_train_batch_size \\\n",
    "                 --per_device_eval_batch_size=$per_device_eval_batch_size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
